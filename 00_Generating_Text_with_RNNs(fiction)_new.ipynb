{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "00_Generating_Text_with_RNNs(fiction).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "___\n",
        "\n",
        "___\n",
        "# Text Generation with Neural Networks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00mwlQwloO5_"
      },
      "source": [
        "# GOOGLE COLLAB USERS ONLY\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBd69MDEm4rF"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDbPtshnm4rL"
      },
      "source": [
        "# IGNORE THE CONTENT OF THIS CELL\n",
        "# tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kycWuRI9oaSP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c66ddb5-b4dc-4e79-bd16-a130ea2066e0"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apj1Chkdm4rS"
      },
      "source": [
        "## Step 1: The Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "source": [
        "path_to_file = 'fiction_bookshelf.txt'"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aavnuByVymwK"
      },
      "source": [
        "text = open(path_to_file, 'r').read()"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "113caab9-541c-47a2-a9a0-e46b0988daac"
      },
      "source": [
        "print(text[:500])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "BOOK ONE\n",
            "THE COMING OF THE MARTIANS\n",
            "\n",
            "\n",
            "\n",
            "I.\n",
            "THE EVE OF THE WAR.\n",
            "\n",
            "No one would have believed in the last years of the nineteenth century\n",
            "that this world was being watched keenly and closely by intelligences\n",
            "greater than manâ€™s and yet as mortal as his own; that as men busied\n",
            "themselves about their various concerns they were scrutinised and\n",
            "studied, perhaps almost as narrowly as a man with a microscope might\n",
            "scrutinise the transient creatures that swarm and multiply in a drop of\n",
            "water. With infinite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXUmR627m4rd"
      },
      "source": [
        "### Understanding unique characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXlNk9SUkLbC"
      },
      "source": [
        "#NEW\n",
        "text2=\"\"\n",
        "specials=['\"',\"'\",\"{\",\"}\",\"[\",\"]\",\"_\",\",\",\".\",\"!\",\"$\",\"\\n\",\" \"]\n",
        "for i in text:\n",
        "  if i in \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\" or i in specials:\n",
        "    text2=text2+i\n",
        "text=text2"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "qdmBrgcUkYky",
        "outputId": "a9704d2c-6188-469e-b2c9-7c606d539cb5"
      },
      "source": [
        "text[:500]"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nBOOK ONE\\nTHE COMING OF THE MARTIANS\\n\\n\\n\\nI.\\nTHE EVE OF THE WAR.\\n\\nNo one would have believed in the last years of the nineteenth century\\nthat this world was being watched keenly and closely by intelligences\\ngreater than mans and yet as mortal as his own that as men busied\\nthemselves about their various concerns they were scrutinised and\\nstudied, perhaps almost as narrowly as a man with a microscope might\\nscrutinise the transient creatures that swarm and multiply in a drop of\\nwater. With infinite c'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9cd178-de1a-454c-9fad-99dfe3d39f5e"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "# new\n",
        "\n",
        "# ***\n",
        "print(vocab)\n",
        "len(vocab)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '\"', '$', \"'\", ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Step 2: Text Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVHmtFndDqy7"
      },
      "source": [
        "We know a neural network can't take in the raw string data, we need to assign numbers to each character.So we create two dictionaries that can go from numeric index to character and character to numeric index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Text Vectorization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IalZLbvOzf-F"
      },
      "source": [
        "char_to_ind = {u:i for i, u in enumerate(vocab)}"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmmP5iCwm4rp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e88f47f9-e087-44de-cd2b-f40e707f3274"
      },
      "source": [
        "char_to_ind"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " '$': 4,\n",
              " \"'\": 5,\n",
              " ',': 6,\n",
              " '.': 7,\n",
              " '0': 8,\n",
              " '1': 9,\n",
              " '2': 10,\n",
              " '3': 11,\n",
              " '4': 12,\n",
              " '5': 13,\n",
              " '6': 14,\n",
              " '7': 15,\n",
              " '8': 16,\n",
              " '9': 17,\n",
              " 'A': 18,\n",
              " 'B': 19,\n",
              " 'C': 20,\n",
              " 'D': 21,\n",
              " 'E': 22,\n",
              " 'F': 23,\n",
              " 'G': 24,\n",
              " 'H': 25,\n",
              " 'I': 26,\n",
              " 'J': 27,\n",
              " 'K': 28,\n",
              " 'L': 29,\n",
              " 'M': 30,\n",
              " 'N': 31,\n",
              " 'O': 32,\n",
              " 'P': 33,\n",
              " 'Q': 34,\n",
              " 'R': 35,\n",
              " 'S': 36,\n",
              " 'T': 37,\n",
              " 'U': 38,\n",
              " 'V': 39,\n",
              " 'W': 40,\n",
              " 'X': 41,\n",
              " 'Y': 42,\n",
              " 'Z': 43,\n",
              " '[': 44,\n",
              " ']': 45,\n",
              " '_': 46,\n",
              " 'a': 47,\n",
              " 'b': 48,\n",
              " 'c': 49,\n",
              " 'd': 50,\n",
              " 'e': 51,\n",
              " 'f': 52,\n",
              " 'g': 53,\n",
              " 'h': 54,\n",
              " 'i': 55,\n",
              " 'j': 56,\n",
              " 'k': 57,\n",
              " 'l': 58,\n",
              " 'm': 59,\n",
              " 'n': 60,\n",
              " 'o': 61,\n",
              " 'p': 62,\n",
              " 'q': 63,\n",
              " 'r': 64,\n",
              " 's': 65,\n",
              " 't': 66,\n",
              " 'u': 67,\n",
              " 'v': 68,\n",
              " 'w': 69,\n",
              " 'x': 70,\n",
              " 'y': 71,\n",
              " 'z': 72,\n",
              " '{': 73,\n",
              " '}': 74}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30ZYaWAOm4rt"
      },
      "source": [
        "ind_to_char = np.array(vocab)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6JPOWwJm4rz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77482721-1e57-4ebb-e4e9-722b926ea2ce"
      },
      "source": [
        "ind_to_char"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\n', ' ', '!', '\"', '$', \"'\", ',', '.', '0', '1', '2', '3', '4',\n",
              "       '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H',\n",
              "       'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U',\n",
              "       'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e',\n",
              "       'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r',\n",
              "       's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}'], dtype='<U1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fhOqV0lm4r2"
      },
      "source": [
        "encoded_text = np.array([char_to_ind[c] for c in text])"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axOX7rFom4r5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee506c7-8811-46f0-a8ea-37234ea2e834"
      },
      "source": [
        "encoded_text"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 19, 32, ...,  0,  0,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "We now have a mapping we can use to go back and forth from characters to numerics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFs1Uza-m4r9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0a14d95-33ad-4cfb-aee6-1c4e716fb152"
      },
      "source": [
        "sample = text[:20]\n",
        "sample"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nBOOK ONE\\nTHE COMING'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIqUCK5Am4sB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06103a7f-a354-4eef-f256-02f9537a9a40"
      },
      "source": [
        "encoded_text[:20]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 19, 32, 32, 28,  1, 32, 31, 22,  0, 37, 25, 22,  1, 20, 32, 30,\n",
              "       26, 31, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "## Step 3: Creating Batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM5vvlOcEUvH"
      },
      "source": [
        "Overall what we are trying to achieve is to have the model predict the next highest probability character given a historical sequence of characters. Its up to us (the user) to choose how long that historic sequence. Too short a sequence and we don't have enough information (e.g. given the letter \"a\" , what is the next character) , too long a sequence and training will take too long and most likely overfit to sequence characters that are irrelevant to characters farther out. While there is no correct sequence length choice, we should consider the text itself, how long normal phrases are in it, and a reasonable idea of what characters/words are relevant to each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAvUYFk7m4sF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7f1250-0a4c-4ebd-f0d8-b3a5fa2980be"
      },
      "source": [
        "print(text[:500])"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "BOOK ONE\n",
            "THE COMING OF THE MARTIANS\n",
            "\n",
            "\n",
            "\n",
            "I.\n",
            "THE EVE OF THE WAR.\n",
            "\n",
            "No one would have believed in the last years of the nineteenth century\n",
            "that this world was being watched keenly and closely by intelligences\n",
            "greater than mans and yet as mortal as his own that as men busied\n",
            "themselves about their various concerns they were scrutinised and\n",
            "studied, perhaps almost as narrowly as a man with a microscope might\n",
            "scrutinise the transient creatures that swarm and multiply in a drop of\n",
            "water. With infinite c\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D45OYgOfm4sJ"
      },
      "source": [
        "line = \"From fairest creatures we desire increase\""
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dKiEVN8m4sL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b45aeef-3f15-4ee4-baca-71b2807df0f8"
      },
      "source": [
        "len(line)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olX67f6-m4sP"
      },
      "source": [
        "part_stanza = \"\"\"From fairest creatures we desire increase,\n",
        "  That thereby beauty's rose might never die,\n",
        "  But as the riper should by time decease,\"\"\""
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qal7MQnqm4sQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a62dbc72-52c2-4a54-9950-d344f2ec53dc"
      },
      "source": [
        "len(part_stanza)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Training Sequences\n",
        "\n",
        "We can use the `tf.data.Dataset.from_tensor_slices` function to convert a text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHJDA39zf-O"
      },
      "source": [
        "seq_len = 120"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VRSK4cOm4sZ"
      },
      "source": [
        "total_num_seq = len(text)//(seq_len+1)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtW0jbbvm4sc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9111af59-fc0e-454a-ab54-b7b9942ac248"
      },
      "source": [
        "total_num_seq"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58929"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciatnowvm4se",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45be8f3a-0082-44e9-c2ec-c949e8d341ac"
      },
      "source": [
        "# Create Training Sequences\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "\n",
        "for i in char_dataset.take(500):\n",
        "     print(ind_to_char[i.numpy()])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "B\n",
            "O\n",
            "O\n",
            "K\n",
            " \n",
            "O\n",
            "N\n",
            "E\n",
            "\n",
            "\n",
            "T\n",
            "H\n",
            "E\n",
            " \n",
            "C\n",
            "O\n",
            "M\n",
            "I\n",
            "N\n",
            "G\n",
            " \n",
            "O\n",
            "F\n",
            " \n",
            "T\n",
            "H\n",
            "E\n",
            " \n",
            "M\n",
            "A\n",
            "R\n",
            "T\n",
            "I\n",
            "A\n",
            "N\n",
            "S\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "I\n",
            ".\n",
            "\n",
            "\n",
            "T\n",
            "H\n",
            "E\n",
            " \n",
            "E\n",
            "V\n",
            "E\n",
            " \n",
            "O\n",
            "F\n",
            " \n",
            "T\n",
            "H\n",
            "E\n",
            " \n",
            "W\n",
            "A\n",
            "R\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "N\n",
            "o\n",
            " \n",
            "o\n",
            "n\n",
            "e\n",
            " \n",
            "w\n",
            "o\n",
            "u\n",
            "l\n",
            "d\n",
            " \n",
            "h\n",
            "a\n",
            "v\n",
            "e\n",
            " \n",
            "b\n",
            "e\n",
            "l\n",
            "i\n",
            "e\n",
            "v\n",
            "e\n",
            "d\n",
            " \n",
            "i\n",
            "n\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "l\n",
            "a\n",
            "s\n",
            "t\n",
            " \n",
            "y\n",
            "e\n",
            "a\n",
            "r\n",
            "s\n",
            " \n",
            "o\n",
            "f\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "n\n",
            "i\n",
            "n\n",
            "e\n",
            "t\n",
            "e\n",
            "e\n",
            "n\n",
            "t\n",
            "h\n",
            " \n",
            "c\n",
            "e\n",
            "n\n",
            "t\n",
            "u\n",
            "r\n",
            "y\n",
            "\n",
            "\n",
            "t\n",
            "h\n",
            "a\n",
            "t\n",
            " \n",
            "t\n",
            "h\n",
            "i\n",
            "s\n",
            " \n",
            "w\n",
            "o\n",
            "r\n",
            "l\n",
            "d\n",
            " \n",
            "w\n",
            "a\n",
            "s\n",
            " \n",
            "b\n",
            "e\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "w\n",
            "a\n",
            "t\n",
            "c\n",
            "h\n",
            "e\n",
            "d\n",
            " \n",
            "k\n",
            "e\n",
            "e\n",
            "n\n",
            "l\n",
            "y\n",
            " \n",
            "a\n",
            "n\n",
            "d\n",
            " \n",
            "c\n",
            "l\n",
            "o\n",
            "s\n",
            "e\n",
            "l\n",
            "y\n",
            " \n",
            "b\n",
            "y\n",
            " \n",
            "i\n",
            "n\n",
            "t\n",
            "e\n",
            "l\n",
            "l\n",
            "i\n",
            "g\n",
            "e\n",
            "n\n",
            "c\n",
            "e\n",
            "s\n",
            "\n",
            "\n",
            "g\n",
            "r\n",
            "e\n",
            "a\n",
            "t\n",
            "e\n",
            "r\n",
            " \n",
            "t\n",
            "h\n",
            "a\n",
            "n\n",
            " \n",
            "m\n",
            "a\n",
            "n\n",
            "s\n",
            " \n",
            "a\n",
            "n\n",
            "d\n",
            " \n",
            "y\n",
            "e\n",
            "t\n",
            " \n",
            "a\n",
            "s\n",
            " \n",
            "m\n",
            "o\n",
            "r\n",
            "t\n",
            "a\n",
            "l\n",
            " \n",
            "a\n",
            "s\n",
            " \n",
            "h\n",
            "i\n",
            "s\n",
            " \n",
            "o\n",
            "w\n",
            "n\n",
            " \n",
            "t\n",
            "h\n",
            "a\n",
            "t\n",
            " \n",
            "a\n",
            "s\n",
            " \n",
            "m\n",
            "e\n",
            "n\n",
            " \n",
            "b\n",
            "u\n",
            "s\n",
            "i\n",
            "e\n",
            "d\n",
            "\n",
            "\n",
            "t\n",
            "h\n",
            "e\n",
            "m\n",
            "s\n",
            "e\n",
            "l\n",
            "v\n",
            "e\n",
            "s\n",
            " \n",
            "a\n",
            "b\n",
            "o\n",
            "u\n",
            "t\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            "i\n",
            "r\n",
            " \n",
            "v\n",
            "a\n",
            "r\n",
            "i\n",
            "o\n",
            "u\n",
            "s\n",
            " \n",
            "c\n",
            "o\n",
            "n\n",
            "c\n",
            "e\n",
            "r\n",
            "n\n",
            "s\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            "y\n",
            " \n",
            "w\n",
            "e\n",
            "r\n",
            "e\n",
            " \n",
            "s\n",
            "c\n",
            "r\n",
            "u\n",
            "t\n",
            "i\n",
            "n\n",
            "i\n",
            "s\n",
            "e\n",
            "d\n",
            " \n",
            "a\n",
            "n\n",
            "d\n",
            "\n",
            "\n",
            "s\n",
            "t\n",
            "u\n",
            "d\n",
            "i\n",
            "e\n",
            "d\n",
            ",\n",
            " \n",
            "p\n",
            "e\n",
            "r\n",
            "h\n",
            "a\n",
            "p\n",
            "s\n",
            " \n",
            "a\n",
            "l\n",
            "m\n",
            "o\n",
            "s\n",
            "t\n",
            " \n",
            "a\n",
            "s\n",
            " \n",
            "n\n",
            "a\n",
            "r\n",
            "r\n",
            "o\n",
            "w\n",
            "l\n",
            "y\n",
            " \n",
            "a\n",
            "s\n",
            " \n",
            "a\n",
            " \n",
            "m\n",
            "a\n",
            "n\n",
            " \n",
            "w\n",
            "i\n",
            "t\n",
            "h\n",
            " \n",
            "a\n",
            " \n",
            "m\n",
            "i\n",
            "c\n",
            "r\n",
            "o\n",
            "s\n",
            "c\n",
            "o\n",
            "p\n",
            "e\n",
            " \n",
            "m\n",
            "i\n",
            "g\n",
            "h\n",
            "t\n",
            "\n",
            "\n",
            "s\n",
            "c\n",
            "r\n",
            "u\n",
            "t\n",
            "i\n",
            "n\n",
            "i\n",
            "s\n",
            "e\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "t\n",
            "r\n",
            "a\n",
            "n\n",
            "s\n",
            "i\n",
            "e\n",
            "n\n",
            "t\n",
            " \n",
            "c\n",
            "r\n",
            "e\n",
            "a\n",
            "t\n",
            "u\n",
            "r\n",
            "e\n",
            "s\n",
            " \n",
            "t\n",
            "h\n",
            "a\n",
            "t\n",
            " \n",
            "s\n",
            "w\n",
            "a\n",
            "r\n",
            "m\n",
            " \n",
            "a\n",
            "n\n",
            "d\n",
            " \n",
            "m\n",
            "u\n",
            "l\n",
            "t\n",
            "i\n",
            "p\n",
            "l\n",
            "y\n",
            " \n",
            "i\n",
            "n\n",
            " \n",
            "a\n",
            " \n",
            "d\n",
            "r\n",
            "o\n",
            "p\n",
            " \n",
            "o\n",
            "f\n",
            "\n",
            "\n",
            "w\n",
            "a\n",
            "t\n",
            "e\n",
            "r\n",
            ".\n",
            " \n",
            "W\n",
            "i\n",
            "t\n",
            "h\n",
            " \n",
            "i\n",
            "n\n",
            "f\n",
            "i\n",
            "n\n",
            "i\n",
            "t\n",
            "e\n",
            " \n",
            "c\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The **batch** method converts these individual character calls into sequences we can feed in as a batch. We use seq_len+1 because of zero indexing. Here is what drop_remainder means:\n",
        "\n",
        "drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
        "    whether the last batch should be dropped in the case it has fewer than\n",
        "    `batch_size` elements; the default behavior is not to drop the smaller\n",
        "    batch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4hkDU3i7ozi"
      },
      "source": [
        "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "Now that we have our sequences, we will perform the following steps for each one to create our target text sequences:\n",
        "\n",
        "1. Grab the input text sequence\n",
        "2. Assign the target text sequence as the input text sequence shifted by one step forward\n",
        "3. Group them together as a tuple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "def create_seq_targets(seq):\n",
        "    input_txt = seq[:-1]\n",
        "    target_txt = seq[1:]\n",
        "    return input_txt, target_txt"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HszljTg8m4so"
      },
      "source": [
        "dataset = sequences.map(create_seq_targets)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkPa7AMrm4sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4a3daf-a5dc-4cd2-86a7-11d12fc6daeb"
      },
      "source": [
        "for input_txt, target_txt in  dataset.take(1):\n",
        "    print(input_txt.numpy())\n",
        "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
        "    print('\\n')\n",
        "    print(target_txt.numpy())\n",
        "    # There is an extra whitespace!\n",
        "    print(''.join(ind_to_char[target_txt.numpy()]))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0 19 32 32 28  1 32 31 22  0 37 25 22  1 20 32 30 26 31 24  1 32 23  1\n",
            " 37 25 22  1 30 18 35 37 26 18 31 36  0  0  0  0 26  7  0 37 25 22  1 22\n",
            " 39 22  1 32 23  1 37 25 22  1 40 18 35  7  0  0 31 61  1 61 60 51  1 69\n",
            " 61 67 58 50  1 54 47 68 51  1 48 51 58 55 51 68 51 50  1 55 60  1 66 54\n",
            " 51  1 58 47 65 66  1 71 51 47 64 65  1 61 52  1 66 54 51  1 60 55 60 51]\n",
            "\n",
            "BOOK ONE\n",
            "THE COMING OF THE MARTIANS\n",
            "\n",
            "\n",
            "\n",
            "I.\n",
            "THE EVE OF THE WAR.\n",
            "\n",
            "No one would have believed in the last years of the nine\n",
            "\n",
            "\n",
            "[19 32 32 28  1 32 31 22  0 37 25 22  1 20 32 30 26 31 24  1 32 23  1 37\n",
            " 25 22  1 30 18 35 37 26 18 31 36  0  0  0  0 26  7  0 37 25 22  1 22 39\n",
            " 22  1 32 23  1 37 25 22  1 40 18 35  7  0  0 31 61  1 61 60 51  1 69 61\n",
            " 67 58 50  1 54 47 68 51  1 48 51 58 55 51 68 51 50  1 55 60  1 66 54 51\n",
            "  1 58 47 65 66  1 71 51 47 64 65  1 61 52  1 66 54 51  1 60 55 60 51 66]\n",
            "BOOK ONE\n",
            "THE COMING OF THE MARTIANS\n",
            "\n",
            "\n",
            "\n",
            "I.\n",
            "THE EVE OF THE WAR.\n",
            "\n",
            "No one would have believed in the last years of the ninet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Generating training batches\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pGotuNzf-S"
      },
      "source": [
        "# Batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Buffer size to shuffle the dataset so it doesn't attempt to shuffle\n",
        "# the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmcCALymm4su",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a642d14c-ca5b-42ac-e09f-a39587465b0e"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Step 4: Creating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "We will use an LSTM based model with a few extra features, including an embedding layer to start off with and **two** LSTM layers. We based this model architecture off the [DeepMoji](https://deepmoji.mit.edu/) and the original source code can be found [here](https://github.com/bfelbo/DeepMoji).\n",
        "\n",
        "The embedding layer will serve as the input layer, which essentially creates a lookup table that maps the numbers indices of each character to a vector with \"embedding dim\" number of dimensions. As you can imagine, the larger this embedding size, the more complex the training. This is similar to the idea behind word2vec, where words are mapped to some n-dimensional space. Embedding before feeding straight into the LSTM usually leads to more realisitic results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embed_dim = 64\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_neurons = 1026"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atb060h5m4s0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeRlEXgym4s1"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcMbIy-xj-w-"
      },
      "source": [
        "### Setting up Loss Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoFVGKlNkJfW"
      },
      "source": [
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sblCzZoslZKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f51739f-364e-4b38-b5fc-ee98f408321d"
      },
      "source": [
        "help(sparse_categorical_crossentropy)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function sparse_categorical_crossentropy in module tensorflow.python.keras.losses:\n",
            "\n",
            "sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1)\n",
            "    Computes the sparse categorical crossentropy loss.\n",
            "    \n",
            "    Standalone usage:\n",
            "    \n",
            "    >>> y_true = [1, 2]\n",
            "    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
            "    >>> loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
            "    >>> assert loss.shape == (2,)\n",
            "    >>> loss.numpy()\n",
            "    array([0.0513, 2.303], dtype=float32)\n",
            "    \n",
            "    Args:\n",
            "      y_true: Ground truth values.\n",
            "      y_pred: The predicted values.\n",
            "      from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n",
            "        we assume that `y_pred` encodes a probability distribution.\n",
            "      axis: (Optional) Defaults to -1. The dimension along which the entropy is\n",
            "        computed.\n",
            "    \n",
            "    Returns:\n",
            "      Sparse categorical crossentropy loss value.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5N4Qxbij5gY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrOOK61Olm1C"
      },
      "source": [
        "def sparse_cat_loss(y_true,y_pred):\n",
        "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtCrdfzEI2N0"
      },
      "source": [
        "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
        "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
        "    # Final Dense Layer to Predict\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
        "    return model"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwsrpOik5zhv"
      },
      "source": [
        "model = create_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embed_dim=embed_dim,\n",
        "  rnn_neurons=rnn_neurons,\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liXuTFYMm4s6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ffb293-37da-4272-e1d3-2dc0be44ccab"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (128, None, 64)           4800      \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (128, None, 1026)         3361176   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (128, None, 75)           77025     \n",
            "=================================================================\n",
            "Total params: 3,443,001\n",
            "Trainable params: 3,443,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Step 5: Training the model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4ygvfHn-wan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f39ef905-8f50-4aab-8ffe-7a150264abb0"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "\n",
        "  # Predict off some random batch\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "\n",
        "  # Display the dimensions of the predictions\n",
        "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")\n"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 120, 75)  <=== (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ld8z3LPBAuv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43afe76b-5d4d-42d4-a178-dc9be73b6a45"
      },
      "source": [
        "example_batch_predictions"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128, 120, 75), dtype=float32, numpy=\n",
              "array([[[ 1.05990935e-03, -4.61204583e-03,  1.57702831e-03, ...,\n",
              "         -3.68429627e-03,  8.26382078e-03, -6.01013890e-04],\n",
              "        [ 6.62855431e-03, -3.83498822e-03, -6.79346034e-03, ...,\n",
              "          9.04543558e-05, -1.72366516e-03,  6.24943757e-04],\n",
              "        [-7.06152059e-06, -6.18229806e-03, -9.45388712e-03, ...,\n",
              "         -3.07954778e-03, -2.73719011e-03,  2.81542679e-03],\n",
              "        ...,\n",
              "        [-2.18794681e-03, -6.27359049e-03, -5.81811834e-03, ...,\n",
              "         -5.01669198e-03, -5.72535675e-03,  1.62856234e-03],\n",
              "        [ 3.24252667e-03,  1.22581923e-03, -5.30428160e-03, ...,\n",
              "         -3.99071630e-03, -4.50633094e-03,  4.40498162e-03],\n",
              "        [ 4.59335651e-03,  2.86829146e-03, -2.14288686e-03, ...,\n",
              "          3.89062427e-03, -3.81574966e-04, -1.07114646e-03]],\n",
              "\n",
              "       [[ 2.57580681e-03,  2.18334724e-04, -3.62313958e-03, ...,\n",
              "          3.80185153e-03,  1.38895353e-04, -3.02373664e-03],\n",
              "        [ 5.23484778e-03, -2.89282249e-03,  3.24597606e-03, ...,\n",
              "          3.79520032e-04,  3.02537181e-03, -4.58335504e-03],\n",
              "        [-3.07117705e-04, -6.11569546e-03, -4.10678610e-03, ...,\n",
              "         -5.79448603e-03, -4.15493036e-04,  5.40644629e-04],\n",
              "        ...,\n",
              "        [-1.39268069e-03,  7.79226189e-04, -4.42103576e-03, ...,\n",
              "         -8.37754272e-03, -5.98793849e-05,  9.54474881e-03],\n",
              "        [ 6.60516508e-03, -1.68508326e-03, -6.39229547e-03, ...,\n",
              "         -7.55766779e-03, -4.90233628e-03,  1.05506182e-02],\n",
              "        [ 7.27196364e-03,  1.00741454e-03, -8.15635547e-03, ...,\n",
              "         -9.63887316e-04, -3.20335198e-03,  3.64922360e-03]],\n",
              "\n",
              "       [[ 1.70200050e-03, -2.74309050e-03,  1.48441386e-03, ...,\n",
              "          3.44846025e-03, -7.29145948e-04, -4.26642224e-03],\n",
              "        [ 4.63937083e-03, -1.98253384e-03, -1.19306042e-03, ...,\n",
              "          4.72802250e-03,  1.22851832e-03, -5.11623546e-03],\n",
              "        [ 3.61000141e-03, -5.91500523e-03,  2.30816123e-03, ...,\n",
              "         -1.94853172e-03,  9.32678767e-03, -3.61360377e-03],\n",
              "        ...,\n",
              "        [ 3.14727332e-03, -3.76156997e-04, -2.08469247e-03, ...,\n",
              "          6.70886878e-03,  1.81442220e-03, -7.85768731e-04],\n",
              "        [ 2.52585881e-03, -5.00523625e-03,  1.01961126e-03, ...,\n",
              "         -5.85167669e-04,  9.02836025e-03, -1.68248988e-03],\n",
              "        [ 5.54406457e-03, -6.95955288e-03,  3.99152096e-03, ...,\n",
              "         -9.41578881e-04,  8.63265991e-03, -3.11541068e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-2.22246768e-03, -4.12123464e-03, -6.38773851e-03, ...,\n",
              "         -4.48551681e-03, -2.16707541e-03,  2.66429153e-03],\n",
              "        [ 3.73181794e-03, -4.06889152e-03,  2.55850959e-04, ...,\n",
              "         -4.59852675e-03,  6.03886554e-04, -1.02511374e-03],\n",
              "        [ 3.80909210e-03, -3.39006481e-04,  1.34218414e-03, ...,\n",
              "          1.92733947e-03,  2.40247929e-03, -2.06851074e-03],\n",
              "        ...,\n",
              "        [ 3.32613417e-05,  2.96963053e-03,  8.16377532e-03, ...,\n",
              "          1.19510703e-02,  6.30081678e-03,  5.24998503e-03],\n",
              "        [ 6.07831031e-03, -2.33958662e-03,  8.52483208e-04, ...,\n",
              "          2.60085193e-03, -1.67481415e-03,  5.78736886e-03],\n",
              "        [ 9.41740163e-03, -4.04994469e-03, -3.52799683e-03, ...,\n",
              "         -3.27062840e-03, -6.81397971e-03,  6.15162216e-03]],\n",
              "\n",
              "       [[ 6.23830035e-03, -2.58981134e-04, -6.41851919e-03, ...,\n",
              "          1.24911743e-03, -6.97799958e-03, -5.15724532e-05],\n",
              "        [-1.52667728e-03,  3.05088796e-03, -3.77164828e-03, ...,\n",
              "          1.90764712e-03, -6.50794525e-03, -5.97036630e-03],\n",
              "        [-2.81298440e-03, -2.47190474e-03, -7.21123302e-03, ...,\n",
              "         -2.57670321e-03, -4.86087147e-03, -6.58851699e-04],\n",
              "        ...,\n",
              "        [ 5.39210625e-03, -4.07534279e-03, -5.35553973e-03, ...,\n",
              "         -3.95252556e-03, -5.43029374e-03,  5.10094780e-03],\n",
              "        [ 3.11270030e-03,  3.95779265e-03, -3.87588562e-03, ...,\n",
              "         -2.07115407e-03, -6.51239185e-04,  4.80927341e-03],\n",
              "        [ 9.29059368e-03,  2.71536387e-03, -7.79805239e-03, ...,\n",
              "         -1.30258501e-04, -8.01464263e-03,  3.86822503e-03]],\n",
              "\n",
              "       [[-2.22246768e-03, -4.12123464e-03, -6.38773851e-03, ...,\n",
              "         -4.48551681e-03, -2.16707541e-03,  2.66429153e-03],\n",
              "        [-4.19577118e-03,  1.58365758e-03, -5.57577144e-03, ...,\n",
              "         -3.05604842e-03, -5.35638444e-03, -3.49018909e-03],\n",
              "        [ 8.92400625e-04,  2.87045771e-03, -1.27256475e-03, ...,\n",
              "          4.54220828e-03, -4.40937933e-04, -3.86536401e-03],\n",
              "        ...,\n",
              "        [ 5.54491719e-03, -2.45353905e-03,  2.08680984e-04, ...,\n",
              "         -2.84732366e-03,  7.00101955e-05, -3.39376694e-03],\n",
              "        [ 1.08575104e-02, -2.73419684e-03, -2.84374319e-03, ...,\n",
              "         -1.07650952e-02, -2.24718754e-03, -3.73087870e-03],\n",
              "        [ 3.12910113e-03, -6.39504427e-03, -6.86162198e-03, ...,\n",
              "         -1.00545455e-02, -2.42296048e-03,  1.19954429e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_achqjT-BGyY"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWrPFk2nBJX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df1eac7-9f4a-4548-f17f-ddbabc775e4e"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(120, 1), dtype=int64, numpy=\n",
              "array([[20],\n",
              "       [68],\n",
              "       [20],\n",
              "       [49],\n",
              "       [45],\n",
              "       [29],\n",
              "       [13],\n",
              "       [22],\n",
              "       [72],\n",
              "       [10],\n",
              "       [32],\n",
              "       [56],\n",
              "       [ 3],\n",
              "       [ 8],\n",
              "       [ 3],\n",
              "       [16],\n",
              "       [16],\n",
              "       [65],\n",
              "       [64],\n",
              "       [53],\n",
              "       [ 6],\n",
              "       [19],\n",
              "       [35],\n",
              "       [46],\n",
              "       [13],\n",
              "       [10],\n",
              "       [ 2],\n",
              "       [21],\n",
              "       [60],\n",
              "       [69],\n",
              "       [10],\n",
              "       [10],\n",
              "       [69],\n",
              "       [59],\n",
              "       [65],\n",
              "       [38],\n",
              "       [ 2],\n",
              "       [10],\n",
              "       [69],\n",
              "       [ 8],\n",
              "       [15],\n",
              "       [73],\n",
              "       [42],\n",
              "       [67],\n",
              "       [48],\n",
              "       [34],\n",
              "       [14],\n",
              "       [62],\n",
              "       [69],\n",
              "       [65],\n",
              "       [28],\n",
              "       [32],\n",
              "       [30],\n",
              "       [ 3],\n",
              "       [23],\n",
              "       [34],\n",
              "       [47],\n",
              "       [36],\n",
              "       [ 3],\n",
              "       [ 0],\n",
              "       [37],\n",
              "       [29],\n",
              "       [20],\n",
              "       [67],\n",
              "       [28],\n",
              "       [ 3],\n",
              "       [ 9],\n",
              "       [27],\n",
              "       [12],\n",
              "       [55],\n",
              "       [ 4],\n",
              "       [28],\n",
              "       [72],\n",
              "       [73],\n",
              "       [64],\n",
              "       [48],\n",
              "       [40],\n",
              "       [35],\n",
              "       [15],\n",
              "       [11],\n",
              "       [27],\n",
              "       [47],\n",
              "       [42],\n",
              "       [72],\n",
              "       [50],\n",
              "       [17],\n",
              "       [66],\n",
              "       [67],\n",
              "       [43],\n",
              "       [40],\n",
              "       [62],\n",
              "       [48],\n",
              "       [59],\n",
              "       [60],\n",
              "       [45],\n",
              "       [23],\n",
              "       [26],\n",
              "       [38],\n",
              "       [17],\n",
              "       [12],\n",
              "       [26],\n",
              "       [62],\n",
              "       [58],\n",
              "       [35],\n",
              "       [25],\n",
              "       [35],\n",
              "       [46],\n",
              "       [65],\n",
              "       [40],\n",
              "       [33],\n",
              "       [27],\n",
              "       [ 7],\n",
              "       [ 8],\n",
              "       [68],\n",
              "       [ 7],\n",
              "       [52],\n",
              "       [45],\n",
              "       [17],\n",
              "       [44],\n",
              "       [70]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi80PQVtBLqj"
      },
      "source": [
        "# Reformat to not be a lists of lists\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qYkIg00-wjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd81937-3c76-4ba9-86a9-5431770f85fc"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([20, 68, 20, 49, 45, 29, 13, 22, 72, 10, 32, 56,  3,  8,  3, 16, 16,\n",
              "       65, 64, 53,  6, 19, 35, 46, 13, 10,  2, 21, 60, 69, 10, 10, 69, 59,\n",
              "       65, 38,  2, 10, 69,  8, 15, 73, 42, 67, 48, 34, 14, 62, 69, 65, 28,\n",
              "       32, 30,  3, 23, 34, 47, 36,  3,  0, 37, 29, 20, 67, 28,  3,  9, 27,\n",
              "       12, 55,  4, 28, 72, 73, 64, 48, 40, 35, 15, 11, 27, 47, 42, 72, 50,\n",
              "       17, 66, 67, 43, 40, 62, 48, 59, 60, 45, 23, 26, 38, 17, 12, 26, 62,\n",
              "       58, 35, 25, 35, 46, 65, 40, 33, 27,  7,  8, 68,  7, 52, 45, 17, 44,\n",
              "       70])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9-P_XqQ_7wY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad008e7-2711-48fe-f968-35af06ff3c73"
      },
      "source": [
        "print(\"Given the input seq: \\n\")\n",
        "print(\"\".join(ind_to_char[input_example_batch[0]]))\n",
        "print('\\n')\n",
        "print(\"Next Char Predictions: \\n\")\n",
        "print(\"\".join(ind_to_char[sampled_indices ]))\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given the input seq: \n",
            "\n",
            "ne day, out by Wandsworth, picking\n",
            "houses to pieces and routing among the wreckage. But they wont keep on\n",
            "doing that. So\n",
            "\n",
            "\n",
            "Next Char Predictions: \n",
            "\n",
            "CvCc]L5Ez2Oj\"0\"88srg,BR_52!Dnw22wmsU!2w07{YubQ6pwsKOM\"FQaS\"\n",
            "TLCuK\"1J4i$Kz{rbWR73JaYzd9tuZWpbmn]FIU94IplRHR_sWPJ.0v.f]9[x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAOE4rzuBh7f"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYDQjKTlm4s8"
      },
      "source": [
        "epochs = 30"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PJ4OVdBm4s8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60bed00b-5cc2-409b-86eb-7d2361545421"
      },
      "source": [
        "model.fit(dataset,epochs=epochs)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "460/460 [==============================] - 73s 154ms/step - loss: 2.8590\n",
            "Epoch 2/30\n",
            "460/460 [==============================] - 74s 159ms/step - loss: 1.7155\n",
            "Epoch 3/30\n",
            "460/460 [==============================] - 74s 159ms/step - loss: 1.4467\n",
            "Epoch 4/30\n",
            "460/460 [==============================] - 74s 158ms/step - loss: 1.3472\n",
            "Epoch 5/30\n",
            "460/460 [==============================] - 73s 157ms/step - loss: 1.2963\n",
            "Epoch 6/30\n",
            "460/460 [==============================] - 74s 159ms/step - loss: 1.2625\n",
            "Epoch 7/30\n",
            "460/460 [==============================] - 74s 158ms/step - loss: 1.2364\n",
            "Epoch 8/30\n",
            "460/460 [==============================] - 74s 159ms/step - loss: 1.2162\n",
            "Epoch 9/30\n",
            "460/460 [==============================] - 75s 160ms/step - loss: 1.1987\n",
            "Epoch 10/30\n",
            "460/460 [==============================] - 75s 160ms/step - loss: 1.1838\n",
            "Epoch 11/30\n",
            "460/460 [==============================] - 74s 159ms/step - loss: 1.1700\n",
            "Epoch 12/30\n",
            "460/460 [==============================] - 74s 159ms/step - loss: 1.1584\n",
            "Epoch 13/30\n",
            "460/460 [==============================] - 75s 161ms/step - loss: 1.1477\n",
            "Epoch 14/30\n",
            "460/460 [==============================] - 75s 160ms/step - loss: 1.1372\n",
            "Epoch 15/30\n",
            "460/460 [==============================] - 74s 159ms/step - loss: 1.1279\n",
            "Epoch 16/30\n",
            "460/460 [==============================] - 75s 160ms/step - loss: 1.1193\n",
            "Epoch 17/30\n",
            "460/460 [==============================] - 75s 160ms/step - loss: 1.1124\n",
            "Epoch 18/30\n",
            "460/460 [==============================] - 74s 159ms/step - loss: 1.1061\n",
            "Epoch 19/30\n",
            "460/460 [==============================] - 74s 158ms/step - loss: 1.0998\n",
            "Epoch 20/30\n",
            "460/460 [==============================] - 75s 160ms/step - loss: 1.0941\n",
            "Epoch 21/30\n",
            "460/460 [==============================] - 75s 160ms/step - loss: 1.0900\n",
            "Epoch 22/30\n",
            "460/460 [==============================] - 74s 159ms/step - loss: 1.0871\n",
            "Epoch 23/30\n",
            "460/460 [==============================] - 75s 160ms/step - loss: 1.0833\n",
            "Epoch 24/30\n",
            "460/460 [==============================] - 75s 161ms/step - loss: 1.0799\n",
            "Epoch 25/30\n",
            "460/460 [==============================] - 74s 159ms/step - loss: 1.0785\n",
            "Epoch 26/30\n",
            "460/460 [==============================] - 74s 159ms/step - loss: 1.0767\n",
            "Epoch 27/30\n",
            "460/460 [==============================] - 75s 160ms/step - loss: 1.0736\n",
            "Epoch 28/30\n",
            "460/460 [==============================] - 75s 160ms/step - loss: 1.0732\n",
            "Epoch 29/30\n",
            "460/460 [==============================] - 74s 159ms/step - loss: 1.0723\n",
            "Epoch 30/30\n",
            "460/460 [==============================] - 75s 160ms/step - loss: 1.0702\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f827eefa550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Step 6: Generating text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYRNG57Govdc"
      },
      "source": [
        "model.save('fiction_gen_new.h5') "
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCoJayFS8H4d"
      },
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iXG3VJvEXWM"
      },
      "source": [
        "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
        "\n",
        "model.load_weights('fiction_gen_new.h5')\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAX3p7_YEilU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f094ba90-12f1-465b-d5a6-e79894fe32b5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (1, None, 64)             4800      \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (1, None, 1026)           3361176   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (1, None, 75)             77025     \n",
            "=================================================================\n",
            "Total params: 3,443,001\n",
            "Trainable params: 3,443,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuwZBX5Ogfd"
      },
      "source": [
        "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
        "  '''\n",
        "  model: Trained Model to Generate Text\n",
        "  start_seed: Intial Seed text in string form\n",
        "  gen_size: Number of characters to generate\n",
        "\n",
        "  Basic idea behind this function is to take in some seed text, format it so\n",
        "  that it is in the correct shape for our network, then loop the sequence as\n",
        "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
        "  time series problems.\n",
        "  '''\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = gen_size\n",
        "\n",
        "  # Vecotrizing starting seed text\n",
        "  input_eval = [char_to_ind[s] for s in start_seed]\n",
        "\n",
        "  # Expand to match batch format shape\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty list to hold resulting generated text\n",
        "  text_generated = []\n",
        "\n",
        "  # Temperature effects randomness in our resulting text\n",
        "  # The term is derived from entropy/thermodynamics.\n",
        "  # The temperature is used to effect probability of next characters.\n",
        "  # Higher probability == lesss surprising/ more expected\n",
        "  # Lower temperature == more surprising / less expected\n",
        " \n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "\n",
        "      # Generate Predictions\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      # Remove the batch shape dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # Use a cateogircal disitribution to select the next character\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # Pass the predicted charracter for the next input\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      # Transform back to character letter\n",
        "      text_generated.append(ind_to_char[predicted_id])\n",
        "\n",
        "  return (start_seed + ''.join(text_generated))"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS69SG5D5lwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61b930a4-496a-4b8c-ce3c-46eb7fcb641b"
      },
      "source": [
        "print(generate_text(model,\"Time\",gen_size=1000))"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time, when it\n",
            "swung forward within an animal.\n",
            "\n",
            "      Then they found it near me.\n",
            "\n",
            "An hour later, sir, as is greater than all that Tardos Mors and\n",
            "Mors Kajous felt for me to reach the comparatively so helpen a few more words\n",
            "hung on and steal the other. Nevertheless, not 'the great white hole in monarcantly.  \"I think we were turning dull\"\n",
            "  like nothing but tendency. It seems a sealed\n",
            "vastne!\n",
            "\n",
            "As I went to the course to the First Line from it.  They wanted to\n",
            "know me about the preceding due depths in a dry human. \"Iom the same\n",
            "places in his provisions and down.  This in search of the master, in the night, but to\n",
            "vicit was to remark the necessary palace of the Zodangan and of\n",
            "all ten years he were\n",
            "just got terrible eyes, a perfect cylinder must have flooded behind them along\n",
            "the refraction, and I have constructed my way behind them.\n",
            "\n",
            "It was not a bit for the last twentyfive million. The expesitor, as unknownwazed\n",
            "and perhaps all knowledge and then we shall find him off in the nearest waters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dff5xCHIeEHO"
      },
      "source": [
        ""
      ],
      "execution_count": 140,
      "outputs": []
    }
  ]
}